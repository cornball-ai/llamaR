#!/usr/bin/env Rscript
#
# llamar - An open-source interactive CLI agent in R
#
# Usage: llamar [options]
#   --provider   anthropic|openai|ollama (default: anthropic)
#   --model      model name (default: provider-specific)
#   --port       MCP server port (default: 7850)
#   --resume   Resume the latest session for this directory
#   --session    Resume a specific session by ID
#   --list       List sessions for this directory
#

# Load .Renviron explicitly (littler may not load it automatically)
renviron <- path.expand("~/.Renviron")
if (file.exists(renviron)) readRenviron(renviron)

suppressPackageStartupMessages({
  library(jsonlite)
  library(llm.api)
  library(llamaR)
})

# ============================================================================
# CLI argument parsing
# ============================================================================

parse_args <- function() {
  args <- commandArgs(trailingOnly = TRUE)

  # Load config for defaults
  config <- llamaR:::load_config(getwd())

  opts <- list(
    provider = config$provider %||% "anthropic",
    model = config$model,
    port = config$port %||% 7850L,
    tools = config$tools,  # NULL = all tools
    context_warn_pct = config$context_warn_pct %||% 75L,
    context_high_pct = config$context_high_pct %||% 90L,
    context_crit_pct = config$context_crit_pct %||% 95L,
    context_compact_pct = config$context_compact_pct %||% 80L,  # Auto-compact threshold
    resume = FALSE,
    session = NULL,
    list = FALSE,
    voice = config$voice$enabled %||% FALSE  # Voice mode
  )

  i <- 1
  while (i <= length(args)) {
    arg <- args[i]
    if (arg == "--provider" && i < length(args)) {
      opts$provider <- args[i + 1]
      i <- i + 2
    } else if (arg == "--model" && i < length(args)) {
      opts$model <- args[i + 1]
      i <- i + 2
    } else if (arg == "--port" && i < length(args)) {
      opts$port <- as.integer(args[i + 1])
      i <- i + 2
    } else if (arg == "--resume" || arg == "-r") {
      opts$resume <- TRUE
      i <- i + 1
    } else if (arg == "--session" && i < length(args)) {
      opts$session <- args[i + 1]
      i <- i + 2
    } else if (arg == "--list") {
      opts$list <- TRUE
      i <- i + 1
    } else if (arg == "--tools" && i < length(args)) {
      opts$tools <- strsplit(args[i + 1], ",")[[1]]
      i <- i + 2
    } else if (arg == "--voice" || arg == "-v") {
      opts$voice <- TRUE
      i <- i + 1
    } else if (arg == "--help" || arg == "-h") {
      cat("
llamar - An open-source interactive CLI agent

Usage: llamar [options]

Options:
  --provider   LLM provider: anthropic, openai, ollama (default: anthropic)
  --model      Model name (default: auto per provider)
  --port       MCP server port (default: 7850)
  --tools      Tool filter: core, file, code, git, r, data, web, chat (default: all)
               Example: --tools core or --tools file,git,code
  --voice      Start with voice mode enabled
  --resume     Resume the latest session for this directory
  --session ID Resume a specific session
  --list       List sessions for this directory
  --help       Show this help

Commands (in chat):
  /quit, /exit   Exit llamar
  /tools         List available tools
  /clear         Clear conversation (keeps session)
  /compact       Summarize conversation to free context
  /sessions      List sessions for this directory
  /context       Show loaded context files
  /model <name>  Switch model
  /provider <p>  Switch provider
  /voice         Toggle voice mode (requires tts.api, stt.api)
  /remember      Store fact with #tags (auto-categorized)
  /recall        Search memories by keyword or tag

Context files (auto-loaded if present):
  README.md      Project description
  PLAN.md        Development roadmap
  fyi.md         Package introspection (from fyi package)
  AGENTS.md      Agent behavior guidelines

Config files:
  ~/.llamar/config.json        Global defaults
  .llamar/config.json          Project overrides

")
      quit(save = "no", status = 0)
    } else {
      i <- i + 1
    }
  }

  opts
}

# ============================================================================
# Server management
# ============================================================================

start_server <- function(port, tools = NULL) {
  # Check if already running
  conn <- tryCatch(
    socketConnection("localhost", port, open = "r+b", blocking = TRUE, timeout = 1),
    error = function(e) NULL
  )

  if (!is.null(conn)) {
    close(conn)
    return(TRUE)  # Already running
  }

  # Start server using llamaR::serve()
  cwd <- getwd()
  tools_arg <- if (!is.null(tools)) {
    sprintf(', tools = c(%s)', paste0('"', tools, '"', collapse = ", "))
  } else ""
  cmd <- sprintf('llamaR::serve(port = %d, cwd = "%s"%s)', port, cwd, tools_arg)

  # Log server output for debugging
  log_file <- file.path(tempdir(), "llamar-server.log")

  if (.Platform$OS.type == "windows") {
    system2("Rscript", c("-e", shQuote(cmd)), wait = FALSE, stdout = log_file, stderr = log_file)
  } else {
    system2("Rscript", c("-e", shQuote(cmd)), wait = FALSE,
            stdout = log_file, stderr = log_file)
  }

  # Wait for startup
  for (i in 1:20) {
    Sys.sleep(0.25)
    conn <- tryCatch(
      socketConnection("localhost", port, open = "r+b", blocking = TRUE, timeout = 1),
      error = function(e) NULL
    )
    if (!is.null(conn)) {
      close(conn)
      return(TRUE)
    }
  }

  FALSE
}

# ============================================================================
# Display helpers
# ============================================================================

color <- list(
  reset = "\033[0m",
  bold = "\033[1m",
  dim = "\033[2m",
  red = "\033[31m",
  green = "\033[32m",
  yellow = "\033[33m",
  blue = "\033[34m",
  magenta = "\033[35m",
  cyan = "\033[36m",
  white = "\033[37m",
  # Bright variants
  bright_red = "\033[91m",
  bright_green = "\033[92m",
  bright_yellow = "\033[93m",
  bright_blue = "\033[94m",
  bright_magenta = "\033[95m",
  bright_cyan = "\033[96m"
)

print_banner <- function(session_id = NULL) {
  cat(sprintf("\n%s llamar%s", color$cyan, color$reset))
  if (!is.null(session_id)) {
    cat(sprintf(" %s[%s]%s", color$dim, session_id, color$reset))
  }
  cat("\n")
  cat(sprintf("%sType /help for commands, /quit to exit%s\n\n", color$dim, color$reset))
}

print_tool <- function(name, args) {
  # Function-style: tool_name("arg1", "arg2")
  if (length(args) > 0) {
    args_str <- paste(sapply(args, function(x) {
      if (is.character(x)) {
        s <- if (nchar(x) > 40) paste0(substr(x, 1, 37), "...") else x
        sprintf('"%s"', s)
      } else {
        as.character(x)
      }
    }), collapse = ", ")
    cat(sprintf("%s%s(%s)%s\n", color$dim, name, args_str, color$reset))
  } else {
    cat(sprintf("%s%s()%s\n", color$dim, name, color$reset))
  }
}

print_result <- function(text) {
  # Minimal output - just show line count, model will summarize
  lines <- strsplit(text, "\n")[[1]]
  n <- length(lines)
  if (n > 1) {
    cat(sprintf("%s(%d lines)%s\n", color$dim, n, color$reset))
  }
  # Single line results shown inline with tool call
}

render_markdown <- function(text) {
  # Bold: **text** or __text__ - bright white
  text <- gsub("\\*\\*([^*]+)\\*\\*", paste0(color$bold, color$white, "\\1", color$reset), text)
  text <- gsub("__([^_]+)__", paste0(color$bold, color$white, "\\1", color$reset), text)

  # Italic: *text* or _text_ (but not inside words)
  text <- gsub("(?<![\\w*])\\*([^*]+)\\*(?![\\w*])", paste0(color$dim, "\\1", color$reset), text, perl = TRUE)
  text <- gsub("(?<![\\w_])_([^_]+)_(?![\\w_])", paste0(color$dim, "\\1", color$reset), text, perl = TRUE)

  # Inline code: `code` - cyan
  text <- gsub("`([^`]+)`", paste0(color$bright_cyan, "\\1", color$reset), text)

  # Headers - colorful
  text <- gsub("(?m)^### (.+)$", paste0(color$bright_blue, "\\1", color$reset), text, perl = TRUE)
  text <- gsub("(?m)^## (.+)$", paste0(color$bold, color$bright_blue, "\\1", color$reset), text, perl = TRUE)
  text <- gsub("(?m)^# (.+)$", paste0(color$bold, color$bright_magenta, "\\1", color$reset), text, perl = TRUE)

  # Bullet points - green bullet
  text <- gsub("(?m)^(\\s*)[-*] ", paste0("\\1", color$green, "â€¢", color$reset, " "), text, perl = TRUE)

  # Links: [text](url) - blue text, dim url
  text <- gsub("\\[([^]]+)\\]\\(([^)]+)\\)", paste0(color$bright_blue, "\\1", color$reset, " ", color$dim, "(\\2)", color$reset), text)

  text
}

print_response <- function(text) {
  rendered <- render_markdown(text)
  cat(sprintf("\n%s%s\n", rendered, color$reset))
}

# ============================================================================
# Context tracking
# ============================================================================

# Model context limits (in tokens)
context_limits <- list(
  # Anthropic
  "claude-sonnet-4-20250514" = 200000L,
  "claude-opus-4-20250514" = 200000L,
  "claude-3-5-sonnet-20241022" = 200000L,
  "claude-3-opus-20240229" = 200000L,
  "claude-3-haiku-20240307" = 200000L,
  # OpenAI
  "gpt-4o" = 128000L,
  "gpt-4o-mini" = 128000L,
  "gpt-4-turbo" = 128000L,
  "gpt-4" = 8192L,
  "gpt-3.5-turbo" = 16385L,
  # Ollama (varies by model)
  "llama3.2" = 128000L,
  "llama3.1" = 128000L,
  "mistral" = 32000L,
  "mixtral" = 32000L,
  "qwen2.5" = 32000L
)

get_context_limit <- function(model) {
  # Try exact match first
  if (!is.null(context_limits[[model]])) {
    return(context_limits[[model]])
  }
  # Try prefix match
  for (name in names(context_limits)) {
    if (startsWith(model, name) || startsWith(name, model)) {
      return(context_limits[[name]])
    }
  }
  # Default
  128000L
}

format_tokens <- function(n) {
  if (n >= 1000000) {
    sprintf("%.1fM", n / 1000000)
  } else if (n >= 1000) {
    sprintf("%.1fK", n / 1000)
  } else {
    as.character(n)
  }
}

print_context_indicator <- function(used, limit, warn_pct = 75, high_pct = 90, crit_pct = 95) {
  pct <- (used / limit) * 100

  # Don't show until we hit warning threshold
  if (pct < warn_pct) return(invisible(NULL))

  used_str <- format_tokens(used)
  limit_str <- format_tokens(limit)

  # Color based on usage: yellow -> orange -> red
  if (pct >= crit_pct) {
    col <- color$bright_red
    warn <- " (consider /clear)"
  } else if (pct >= high_pct) {
    col <- "\033[38;5;208m"  # Orange (256-color)
    warn <- ""
  } else {
    col <- color$yellow
    warn <- ""
  }

  cat(sprintf("%s[%s / %s tokens %.0f%%]%s%s\n",
              col, used_str, limit_str, pct, warn, color$reset))
}

# ============================================================================
# Auto-compaction
# ============================================================================

compact_prompt <- '
Summarize this conversation concisely, preserving:
1. What was accomplished (completed tasks, files modified)
2. Current work in progress
3. Key decisions and constraints mentioned
4. Pending tasks or next steps
5. Any errors encountered and their resolution

Be specific about file names, function names, and technical details.
Format as a structured summary the assistant can use to continue the work.
'

do_compact <- function(session, provider, model, system_prompt) {
  cat(sprintf("%s\nAuto-compacting conversation...%s\n", color$cyan, color$reset))

  # Build conversation text from messages
  conv_text <- vapply(session$messages, function(m) {
    sprintf("[%s]: %s", m$role, m$content)
  }, character(1))
  conv_text <- paste(conv_text, collapse = "\n\n")

  # Ask LLM to summarize
  summary_prompt <- sprintf("%s\n\n---\nConversation to summarize:\n%s", compact_prompt, conv_text)

  result <- tryCatch({
    llm.api::chat(
      prompt = summary_prompt,
      provider = provider,
      model = model,
      system = "You are a helpful assistant that creates concise conversation summaries.",
      temperature = 0.3
    )
  }, error = function(e) {
    cat(sprintf("%sCompaction failed: %s%s\n", color$bright_magenta, e$message, color$reset))
    NULL
  })

  if (is.null(result)) {
    return(NULL)
  }

  summary <- result$content

  # Estimate tokens in summary (~4 chars per token)
  new_tokens <- as.integer(nchar(summary) / 4)

  cat(sprintf("%sCompacted to ~%s tokens%s\n", color$green, format_tokens(new_tokens), color$reset))

  list(summary = summary, tokens = new_tokens)
}


# ============================================================================
# Tool output buffer (for /last command)
# ============================================================================

# Store last N tool outputs
.tool_buffer <- new.env(parent = emptyenv())
.tool_buffer$outputs <- list()
.tool_buffer$max_size <- 20L

tool_buffer_add <- function(name, args, result) {
  entry <- list(
    name = name,
    args = args,
    result = result,
    time = Sys.time()
  )
  .tool_buffer$outputs <- c(list(entry), .tool_buffer$outputs)
  # Trim to max size
  if (length(.tool_buffer$outputs) > .tool_buffer$max_size) {
    .tool_buffer$outputs <- .tool_buffer$outputs[1:.tool_buffer$max_size]
  }
}

tool_buffer_get <- function(n = 1) {
  if (n > length(.tool_buffer$outputs)) {
    return(NULL)
  }
  .tool_buffer$outputs[[n]]
}

tool_buffer_list <- function() {
  .tool_buffer$outputs
}

# ============================================================================
# Tool approval system
# ============================================================================

# Check if a tool requires approval
requires_approval <- function(tool_name, dangerous_tools) {
  tool_name %in% dangerous_tools
}

# Load approvals from project-local file
load_approvals <- function(cwd) {
  path <- file.path(cwd, ".llamar", "approvals.json")
  if (file.exists(path)) {
    tryCatch(
      jsonlite::fromJSON(path, simplifyVector = FALSE),
      error = function(e) list()
    )
  } else {
    list()
  }
}

# Save approval to project-local file
save_approval <- function(cwd, tool_name) {
  approvals <- load_approvals(cwd)
  approvals[[tool_name]] <- TRUE

  path <- file.path(cwd, ".llamar", "approvals.json")
  dir.create(dirname(path), showWarnings = FALSE, recursive = TRUE)
  jsonlite::write_json(approvals, path, auto_unbox = TRUE, pretty = TRUE)
}

# Check if tool is already approved for this project
is_approved <- function(cwd, tool_name) {
  approvals <- load_approvals(cwd)
  isTRUE(approvals[[tool_name]])
}

# Ask user for approval
ask_approval <- function(name, args) {
  cat(sprintf("\n%s%s wants to execute:%s\n", color$yellow, name, color$reset))
  print_tool(name, args)

  # Show numbered options
  cat(sprintf("%s1) allow once  2) allow always  3) deny%s\n", color$dim, color$reset))

  # Use bash read -e for readline support
  response <- system(
    sprintf('bash -c \'read -e -p "%sChoice [1]: %s" r && echo "$r"\'', color$yellow, color$reset),
    intern = TRUE
  )
  if (length(response) == 0) response <- ""
  response <- trimws(response)

  # Default to "1" (allow once) if empty
  if (response == "") response <- "1"

  list(
    approved = response %in% c("1", "2"),
    always = response == "2"
  )
}

# ============================================================================
# Main agent loop
# ============================================================================

run_agent <- function(opts) {
  cwd <- getwd()

  # Handle --list (exit early)
  if (isTRUE(opts$list)) {
    sessions <- llamaR:::session_list(cwd)
    cat(llamaR:::format_session_list(sessions), "\n")
    return(invisible(NULL))
  }

  # Load or create session
  session <- NULL

  if (!is.null(opts$session)) {
    # Resume specific session
    session <- llamaR:::session_load(opts$session, cwd)
    if (is.null(session)) {
      cat(sprintf("%sSession not found: %s%s\n", color$yellow, opts$session, color$reset))
      return(invisible(NULL))
    }
    # Use session's provider/model unless overridden
    if (is.null(opts$model)) opts$model <- session$model
    opts$provider <- session$provider %||% opts$provider
  } else if (opts$resume) {
    # Resume latest session
    session <- llamaR:::session_latest(cwd)
    if (is.null(session)) {
      cat(sprintf("%sNo sessions found. Starting new session.%s\n", color$dim, color$reset))
    } else {
      # Use session's provider/model unless overridden
      if (is.null(opts$model)) opts$model <- session$model
      opts$provider <- session$provider %||% opts$provider
    }
  }

  # Create new session if needed
  if (is.null(session)) {
    session <- llamaR:::session_new(opts$provider, opts$model, cwd)
  }

  print_banner(session$id)

  # Start/connect to MCP server
  server_log <- file.path(tempdir(), "llamar-server.log")
  cat(sprintf("%sStarting MCP server on port %d...%s\n", color$dim, opts$port, color$reset))
  cat(sprintf("%sServer log: %s%s\n", color$dim, server_log, color$reset))

  if (!start_server(opts$port, opts$tools)) {
    cat(sprintf("%sError: Could not start MCP server%s\n", color$bright_magenta, color$reset))
    cat("Make sure llamaR package is installed.\n")
    return(invisible(NULL))
  }

  # Connect
  conn <- tryCatch(
    mcp_connect(port = opts$port, name = "llamar"),
    error = function(e) {
      cat(sprintf("%sError connecting to server: %s%s\n", color$bright_magenta, e$message, color$reset))
      NULL
    }
  )

  if (is.null(conn)) return(invisible(NULL))

  cat(sprintf("%sConnected. %d tools available.%s\n\n",
              color$dim, length(conn$tools), color$reset))

  # Load skill docs (SKILL.md files)
  llamaR:::load_skill_docs(path.expand("~/.llamar/skills"))
  llamaR:::load_skill_docs(file.path(cwd, ".llamar", "skills"))
  skill_docs <- llamaR:::list_skill_docs()

  # Load context files
  context_files <- llamaR:::list_context_files(cwd)
  system_prompt <- llamaR:::load_context(cwd)

  if (length(context_files) > 0 || length(skill_docs) > 0) {
    cat(sprintf("%sLoaded context:%s\n", color$dim, color$reset))
    for (f in context_files) {
      cat(sprintf("  %s%s%s\n", color$cyan, basename(f), color$reset))
    }
    if (length(skill_docs) > 0) {
      cat(sprintf("  %s%d skill(s): %s%s\n", color$cyan, length(skill_docs),
                  paste(skill_docs, collapse = ", "), color$reset))
    }
    cat("\n")
  }

  # Get tools for LLM
  tools <- mcp_tools_for_api(conn)

  # Get approval settings from config
  config <- llamaR:::load_config(cwd)
  approval_mode <- config$approval_mode %||% "ask"
  dangerous_tools <- config$dangerous_tools %||% c("bash", "run_r", "run_r_script", "write_file")

  # Tool handler (captures cwd, approval_mode, dangerous_tools in closure)
  tool_handler <- function(name, args) {
    # Check if approval needed (only in "ask" mode)
    if (approval_mode == "ask" && requires_approval(name, dangerous_tools)) {
      if (!is_approved(cwd, name)) {
        approval <- ask_approval(name, args)
        if (!approval$approved) {
          cat(sprintf("%sDenied.%s\n", color$dim, color$reset))
          return("Tool execution denied by user")
        }
        if (approval$always) {
          save_approval(cwd, name)
          cat(sprintf("%sApproved for this project%s\n", color$dim, color$reset))
        }
      }
    } else if (approval_mode == "deny" && requires_approval(name, dangerous_tools)) {
      cat(sprintf("%sDenied (approval_mode=deny).%s\n", color$dim, color$reset))
      return("Tool execution denied by configuration")
    }

    # Brief indicator for tool execution
    cat(sprintf("%s%s...%s ", color$dim, name, color$reset))

    failed <- FALSE
    result <- tryCatch({
      mcp_call(conn, name, args)
    }, error = function(e) {
      # Try to reconnect on connection errors
      if (grepl("closed connection|SIGPIPE", e$message)) {
        cat(sprintf("%sreconnecting...%s ", color$dim, color$reset))
        conn <<- tryCatch(
          mcp_connect(port = opts$port, name = "llamar"),
          error = function(e2) NULL
        )
        if (!is.null(conn)) {
          # Retry the call
          return(tryCatch(
            mcp_call(conn, name, args),
            error = function(e2) {
              failed <<- TRUE
              cat(sprintf("%sfailed: %s%s\n", color$bright_magenta, e2$message, color$reset))
              list(text = paste("Error:", e2$message))
            }
          ))
        }
      }
      failed <<- TRUE
      cat(sprintf("%sfailed: %s%s\n", color$bright_magenta, e$message, color$reset))
      list(text = paste("Error:", e$message))
    })

    text <- result$text %||% ""
    if (!failed) {
      lines <- length(strsplit(text, "\n")[[1]])
      cat(sprintf("%s(%d lines)%s\n", color$dim, lines, color$reset))
      # Store in buffer for /last command
      tool_buffer_add(name, args, text)
    }
    text
  }

  # Conversation state - restore from session
  provider <- opts$provider
  model <- opts$model

  # Display model
  display_model <- model %||% switch(provider,
    anthropic = "claude-sonnet-4-20250514",
    openai = "gpt-4o",
    ollama = "llama3.2"
  )

  cat(sprintf("%s%s @ %s%s\n", color$dim, display_model, provider, color$reset))

  # Voice mode state
  voice_mode <- opts$voice %||% FALSE
  if (voice_mode) {
    # Check if voice is available
    if (!llamaR:::voice_can_enable(config)) {
      status <- llamaR:::voice_status(config)
      cat(sprintf("%sVoice mode unavailable: %s%s\n", color$yellow, status, color$reset))
      voice_mode <- FALSE
    } else {
      status <- llamaR:::voice_status(config)
      cat(sprintf("%sVoice mode enabled (%s)%s\n", color$green, status, color$reset))
      cat(sprintf("%sPress Enter to speak, type /voice to disable%s\n", color$dim, color$reset))
    }
  }

  # Track token usage for context indicator
  session_tokens <- session$tokens %||% 0L
  context_limit <- get_context_limit(display_model)

  # Show resumed message count
  if (length(session$messages) > 0) {
    cat(sprintf("%sResumed session with %d messages.%s\n",
                color$dim, length(session$messages), color$reset))
    if (session_tokens > 0) {
      print_context_indicator(session_tokens, context_limit,
                              opts$context_warn_pct, opts$context_high_pct, opts$context_crit_pct)
    }
  }

  # REPL
  while (TRUE) {
    # Read input - voice mode or text mode
    if (voice_mode) {
      # Voice mode: show prompt, wait for Enter, then record
      cat(sprintf("%s[voice]%s ", color$magenta, color$reset))
      # Wait for Enter to start recording
      ready <- system(
        sprintf('bash -c \'read -e -p "%s> %s" line && echo "$line"\'', color$green, color$reset),
        intern = TRUE
      )

      # Handle EOF
      if (length(ready) == 0) {
        cat("\n")
        break
      }

      # Check for commands typed instead of pressing Enter
      ready <- trimws(ready)
      if (nchar(ready) > 0 && startsWith(ready, "/")) {
        prompt <- ready  # Process as command
      } else {
        # Record and transcribe
        prompt <- llamaR:::voice_listen(config, show_status = TRUE)
        if (is.null(prompt) || nchar(trimws(prompt)) == 0) {
          next
        }
      }
    } else {
      # Text mode: standard readline input
      prompt <- system(
        sprintf('bash -c \'read -e -p "%s> %s" line && echo "$line"\'', color$green, color$reset),
        intern = TRUE
      )

      # Handle EOF (Ctrl+D)
      if (length(prompt) == 0) {
        cat("\n")
        break
      }

      prompt <- trimws(prompt)
      if (nchar(prompt) == 0) next
    }

    # Handle commands
    if (startsWith(prompt, "/")) {
      cmd_parts <- strsplit(prompt, "\\s+")[[1]]
      cmd <- tolower(cmd_parts[1])
      cmd_arg <- if (length(cmd_parts) > 1) cmd_parts[2] else NULL

      if (cmd %in% c("/quit", "/exit", "/q")) {
        break
      } else if (cmd == "/tools") {
        cat(sprintf("\n%sAvailable tools:%s\n", color$bold, color$reset))
        for (tool in conn$tools) {
          cat(sprintf("  %s%s%s - %s\n", color$cyan, tool$name, color$reset,
                      tool$description %||% ""))
        }
        cat("\n")
        next
      } else if (cmd == "/clear") {
        session$messages <- list()
        session$tokens <- 0L
        session_tokens <- 0L
        llamaR:::session_save(session)
        cat(sprintf("%sConversation cleared.%s\n", color$dim, color$reset))
        next
      } else if (cmd == "/compact") {
        if (length(session$messages) < 2) {
          cat(sprintf("%sNothing to compact.%s\n", color$dim, color$reset))
        } else {
          compact_result <- do_compact(session, provider, display_model, system_prompt)
          if (!is.null(compact_result)) {
            session$messages <- list(
              list(role = "user", content = "[Previous conversation summary]"),
              list(role = "assistant", content = compact_result$summary)
            )
            session_tokens <- compact_result$tokens
            session$tokens <- session_tokens
            session$compactions <- (session$compactions %||% 0L) + 1L
            llamaR:::session_save(session)
          }
        }
        next
      } else if (cmd == "/sessions") {
        sessions <- llamaR:::session_list(cwd)
        cat(llamaR:::format_session_list(sessions), "\n")
        next
      } else if (cmd == "/model" && !is.null(cmd_arg)) {
        model <- cmd_arg
        session$model <- model
        llamaR:::session_save(session)
        cat(sprintf("%sModel set to: %s%s\n", color$dim, model, color$reset))
        next
      } else if (cmd == "/provider" && !is.null(cmd_arg)) {
        provider <- cmd_arg
        session$provider <- provider
        llamaR:::session_save(session)
        cat(sprintf("%sProvider set to: %s%s\n", color$dim, provider, color$reset))
        next
      } else if (cmd == "/voice") {
        # Toggle voice mode
        if (voice_mode) {
          voice_mode <- FALSE
          cat(sprintf("%sVoice mode disabled%s\n", color$dim, color$reset))
        } else {
          if (!llamaR:::voice_can_enable(config)) {
            status <- llamaR:::voice_status(config)
            cat(sprintf("%sVoice mode unavailable: %s%s\n", color$yellow, status, color$reset))
          } else {
            voice_mode <- TRUE
            status <- llamaR:::voice_status(config)
            cat(sprintf("%sVoice mode enabled (%s)%s\n", color$green, status, color$reset))
            cat(sprintf("%sPress Enter to speak, type /voice to disable%s\n", color$dim, color$reset))
          }
        }
        next
      } else if (cmd == "/context") {
        if (length(context_files) == 0) {
          cat(sprintf("%sNo context files loaded.%s\n", color$dim, color$reset))
          cat("Create LLAMAR.md or fyi.md in this directory to add context.\n")
        } else {
          cat(sprintf("%sLoaded context files:%s\n", color$bold, color$reset))
          for (f in context_files) {
            cat(sprintf("  %s%s%s\n", color$cyan, f, color$reset))
          }
        }
        next
      } else if (cmd == "/remember") {
        # /remember <fact> #tags - Add to project memory with auto-categorization
        # /remember --global <fact> #tags - Add to global memory
        if (length(cmd_parts) < 2) {
          cat("Usage: /remember <fact> #optional #tags\n")
          cat("       /remember --global <fact> #tags\n")
          next
        }

        fact_text <- paste(cmd_parts[-1], collapse = " ")
        is_global <- startsWith(fact_text, "--global ")

        if (is_global) {
          fact_text <- sub("^--global ", "", fact_text)
          scope <- "global"
        } else {
          scope <- "project"
        }

        # Use the memory module for storage (handles tags, categorization)
        tryCatch({
          llamaR:::memory_store(fact_text, scope = scope, cwd = cwd)
          clean_fact <- llamaR:::strip_tags(fact_text)
          tags <- llamaR:::parse_tags(fact_text)
          tag_str <- if (length(tags) > 0) sprintf(" [%s]", paste0("#", tags, collapse = " ")) else ""
          scope_str <- if (scope == "global") " (global)" else ""
          cat(sprintf("%sRemembered%s: %s%s%s\n", color$green, scope_str, clean_fact, tag_str, color$reset))
        }, error = function(e) {
          cat(sprintf("%sError: %s%s\n", color$red, e$message, color$reset))
        })

        # Reload context to pick up new memory
        system_prompt <- llamaR:::load_context(cwd)
        next
      } else if (cmd == "/recall") {
        # /recall <query> - Search memory
        # /recall --tags - List all tags
        if (length(cmd_parts) < 2) {
          cat("Usage: /recall <query>   Search for memories\n")
          cat("       /recall --tags    List all memory tags\n")
          next
        }

        query <- paste(cmd_parts[-1], collapse = " ")

        if (query == "--tags") {
          tags <- llamaR:::memory_list_tags(scope = "both", cwd = cwd)
          if (length(tags) == 0) {
            cat(sprintf("%sNo tags found.%s\n", color$dim, color$reset))
          } else {
            cat(sprintf("%sMemory tags:%s\n", color$bold, color$reset))
            cat(sprintf("  %s#%s%s\n", color$cyan, paste(tags, collapse = paste0(color$reset, ", ", color$cyan, "#")), color$reset))
          }
          next
        }

        # Search memory
        results <- llamaR:::memory_search(query, scope = "both", cwd = cwd)
        if (length(results) == 0) {
          cat(sprintf("%sNo memories matching '%s'%s\n", color$dim, query, color$reset))
        } else {
          cat(sprintf("\n%sFound %d memor%s:%s\n", color$bold, length(results),
                      if (length(results) == 1) "y" else "ies", color$reset))
          formatted <- llamaR:::format_memory_results(results)
          cat(formatted, "\n")
        }
        next
      } else if (cmd == "/last") {
        # /last [N] - show tool output (default: most recent)
        n <- if (!is.null(cmd_arg)) as.integer(cmd_arg) else 1L
        outputs <- tool_buffer_list()

        if (length(outputs) == 0) {
          cat(sprintf("%sNo tool outputs yet.%s\n", color$dim, color$reset))
          next
        }

        if (n < 1 || n > length(outputs)) {
          cat(sprintf("%sInvalid index. Have %d outputs.%s\n", color$yellow, length(outputs), color$reset))
          next
        }

        entry <- outputs[[n]]
        cat(sprintf("\n%s%s%s @ %s\n", color$cyan, entry$name, color$reset,
                    format(entry$time, "%H:%M:%S")))
        if (length(entry$args) > 0) {
          cat(sprintf("%sArgs: %s%s\n", color$dim,
                      jsonlite::toJSON(entry$args, auto_unbox = TRUE), color$reset))
        }
        cat(sprintf("%s%s%s\n", color$dim, strrep("-", 40), color$reset))
        cat(entry$result)
        cat("\n")
        next
      } else if (cmd == "/outputs") {
        # List recent tool outputs
        outputs <- tool_buffer_list()
        if (length(outputs) == 0) {
          cat(sprintf("%sNo tool outputs yet.%s\n", color$dim, color$reset))
        } else {
          cat(sprintf("\n%sRecent tool outputs:%s\n", color$bold, color$reset))
          for (i in seq_along(outputs)) {
            entry <- outputs[[i]]
            lines <- length(strsplit(entry$result, "\n")[[1]])
            cat(sprintf("  %s[%d]%s %s%s%s (%d lines) @ %s\n",
                        color$dim, i, color$reset,
                        color$cyan, entry$name, color$reset,
                        lines, format(entry$time, "%H:%M:%S")))
          }
          cat(sprintf("\n%sUse /last [N] to view output%s\n", color$dim, color$reset))
        }
        next
      } else if (cmd == "/help") {
        cat("
Commands:
  /quit, /exit   Exit llamar
  /tools         List available tools
  /clear         Clear conversation (keeps session)
  /compact       Summarize conversation to free context
  /sessions      List sessions for this directory
  /context       Show loaded context files
  /model <name>  Switch model
  /provider <p>  Switch provider (anthropic, openai, ollama)
  /voice         Toggle voice mode (requires tts.api, stt.api)

Memory:
  /remember <fact> #tags      Remember with auto-categorization
  /remember --global <fact>   Remember to global memory
  /recall <query>             Search memories
  /recall --tags              List all memory tags

Tool output:
  /last [N]      Show tool output (1=most recent)
  /outputs       List recent tool outputs
  /help          Show this help

")
        next
      } else {
        cat(sprintf("%sUnknown command: %s%s\n", color$yellow, cmd, color$reset))
        next
      }
    }

    # Add user message to session
    session <- llamaR:::session_add_message(session, "user", prompt)
    llamaR:::session_save(session)

    # Send to LLM with tools
    # Check API key (not needed for Ollama)
    if (provider != "ollama") {
      .config <- llm.api:::.get_provider_config(provider)
      api_key <- .config$api_key %||% ""
      if (nchar(api_key) == 0) {
        cat(sprintf("%sWarning: No API key found for %s%s\n", color$yellow, provider, color$reset))
        env_var <- switch(provider,
          anthropic = "ANTHROPIC_API_KEY",
          openai = "OPENAI_API_KEY",
          paste0(toupper(provider), "_API_KEY")
        )
        cat(sprintf("Set %s in ~/.Renviron\n", env_var))
        next
      }
    }

    # Strip extra fields from messages for API (only role + content)
    api_history <- lapply(session$messages, function(m) {
      list(role = m$role, content = m$content)
    })

    result <- tryCatch({
      agent(
        prompt = prompt,
        tools = tools,
        tool_handler = tool_handler,
        system = system_prompt,
        model = model,
        provider = provider,
        history = api_history,
        verbose = FALSE  # We handle display ourselves
      )
    }, error = function(e) {
      cat(sprintf("%sError: %s%s\n", color$bright_magenta, e$message, color$reset))
      NULL
    })

    if (!is.null(result)) {
      print_response(result$content)

      # Speak response in voice mode
      if (voice_mode) {
        llamaR:::voice_speak(result$content, config, show_status = TRUE)
      }

      # Update token count and show context indicator
      if (!is.null(result$usage)) {
        session_tokens <- session_tokens + result$usage$total_tokens
        session$tokens <- session_tokens
        print_context_indicator(session_tokens, context_limit,
                                opts$context_warn_pct, opts$context_high_pct, opts$context_crit_pct)

        # Auto-compact if approaching limit
        pct <- (session_tokens / context_limit) * 100
        if (pct >= opts$context_compact_pct && length(session$messages) > 2) {
          compact_result <- do_compact(session, provider, display_model, system_prompt)
          if (!is.null(compact_result)) {
            # Replace session with summary
            session$messages <- list(
              list(role = "user", content = "[Previous conversation summary]"),
              list(role = "assistant", content = compact_result$summary)
            )
            session_tokens <- compact_result$tokens
            session$tokens <- session_tokens
            session$compactions <- (session$compactions %||% 0L) + 1L
          }
        }
      }

      # Add assistant response to session
      session <- llamaR:::session_add_message(session, "assistant", result$content)
      llamaR:::session_save(session)
    }
  }

  # Final save
  llamaR:::session_save(session)

  # Cleanup
  cat(sprintf("\n%sGoodbye.%s\n", color$dim, color$reset))
  suppressWarnings(mcp_close(conn))
}

# ============================================================================
# Entry point
# ============================================================================

if (!interactive()) {
  opts <- parse_args()
  run_agent(opts)
}
